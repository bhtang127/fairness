{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a minimax game between a data pre-processor $G$ and a discriminative decision maker $H$.\n",
    "\n",
    "$H$ want to discriminate people from protected label, but as a decision maker, he must\n",
    "\n",
    "* Use only the data processed by $G$\n",
    "* Not get a too bad prediction\n",
    "\n",
    "On the other side $G$ want to pre-process the data so that $H$ can not discriminate it easily among the restriction mentioned above. We add a further restriction that $G$ can not use the protected labels, but he may access other similiar data with them. Therefore in the whole process of decision making, we never use protected labels. We can then write this game as:\n",
    "\n",
    "$$ \\min_G \\max_H \\text{Loss}\\{cov[e(X), H(G(X))]\\} + \\text{Acc}\\{H(G(X))\\} $$\n",
    "\n",
    "Where here e(X) is the propensity score estimated using other similiar data, here we assume it will not change under the minimax game. As we mentioned before, $cov[e(X), H(G(X))]$ is a measure of the fairness of $H$ under statistical parity. Then the `Loss` is an arbitrary loss and `ACC` a measure of accuracy of $H$. Here we use 2-norm for the loss and negetive cross-entropy for the accuracy.\n",
    "\n",
    "We can train this adversarial learning problem as step optimization as below:\n",
    "\n",
    "For $G$ it do:\n",
    "$$ \\min_G || cov[e(X), H(G(X))] ||^2 $$\n",
    "\n",
    "Then for $H$ it do:\n",
    "$$ \\max_H \\lambda || cov[e(X), H(G(X))] ||^2 + E[Y\\log(H(G(X))) + (1-Y)\\log(1 - H(G(X)))]$$\n",
    "\n",
    "We use this idea to do experiments as below: (data from [UCI Machine Learning Repo: default of credit card clients Data Set](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(\"../data/credit_default.xls\")\n",
    "data = xls.parse('Data', skiprows=1, index_col=None)\n",
    "data = np.asarray(data)[:,1:]\n",
    "default = data[:,-1]\n",
    "sex = data[:,1]\n",
    "credit = np.delete(data,[1,23],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we standardized the data and split to three sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9900, 22) (13500, 22) (5100, 22)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection as sk\n",
    "\n",
    "credit_mean = np.mean(credit, axis=0)\n",
    "credit_std = np.std(credit, axis=0)\n",
    "credit = (credit - credit_mean) / credit_std\n",
    "\n",
    "credit_train, credit_test, label_train, label_test = sk.train_test_split( credit,\n",
    "                                                                          np.vstack([sex,default]).T,\n",
    "                                                                          test_size=0.5,\n",
    "                                                                          random_state=42 )\n",
    "\n",
    "credit_train, credit_val, label_train, label_val = sk.train_test_split(credit_train, \n",
    "                                                                       label_train, \n",
    "                                                                       test_size=0.1, \n",
    "                                                                       random_state=42)\n",
    "\n",
    "credit_test, credit_dis, label_test, label_dis = sk.train_test_split(credit_test, \n",
    "                                                                       label_test, \n",
    "                                                                       test_size=0.66, \n",
    "                                                                       random_state=42)\n",
    "\n",
    "print(credit_dis.shape, credit_train.shape, credit_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we train the propensity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900, 22)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_dis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5981 3919\n"
     ]
    }
   ],
   "source": [
    "print(sum(label_dis[:,0]-1==1), sum(label_dis[:,0]-1==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6035317860746721"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5981 / 9910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Activation\n",
    "\n",
    "with tf.Session() as session:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=22, activation=\"relu\"))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(credit_dis, label_dis[:,0]-1, epochs=100, verbose=0)\n",
    "\n",
    "    p_scores = model.predict(credit_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13500, 1)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_scores = np.array(p_scores)\n",
    "p_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Compute the leaky ReLU activation function.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor with arbitrary shape\n",
    "    - alpha: leak parameter for leaky ReLU\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor with the same shape as x\n",
    "    \"\"\"\n",
    "    # TODO: implement leaky ReLU\n",
    "    act = alpha * tf.minimum(x, 0) + tf.maximum(x, 0)\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "    \"\"\"Compute discriminator score for a batch of input images.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor of shape [batch_size, 16]\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor with shape [batch_size, 1], containing the score \n",
    "    for an image being real for each input image.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "        # TODO: implement architecture\n",
    "        h1 = tf.layers.dense(x, 64)\n",
    "        r1 = leaky_relu(h1, 0.01)\n",
    "        h2 = tf.layers.dense(r1, 64)\n",
    "        r2 = leaky_relu(h2, 0.01)\n",
    "        logits = tf.layers.dense(r2, 1)\n",
    "        return logits\n",
    "    \n",
    "def generator(z):\n",
    "    \"\"\"Generate images from a random noise vector.\n",
    "    \n",
    "    Inputs:\n",
    "    - z: TensorFlow Tensor of random noise with shape [batch_size, noise_dim]\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor of generated images, with shape [batch_size, 16].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        # TODO: implement architecture\n",
    "        h1 = tf.layers.dense(z, 64)\n",
    "        r1 = tf.nn.relu(h1)\n",
    "        h2 = tf.layers.dense(r1, 64)\n",
    "        r2 = tf.nn.relu(h2)\n",
    "        proc = tf.layers.dense(r2, 16, activation=tf.tanh)\n",
    "        return proc\n",
    "    \n",
    "def gan_loss(p_scores, labels, logits, lam):\n",
    "    \"\"\"Compute the GAN loss.\n",
    "    - D_loss: discriminator loss scalar\n",
    "    - G_loss: generator loss scalar\n",
    "    \"\"\"\n",
    "    # TODO: compute D_loss and G_loss\n",
    "    G_loss = tf.reduce_mean(p_scores * tf.nn.sigmoid(logits)) - \\\n",
    "             tf.reduce_mean(p_scores) * tf.reduce_mean(tf.nn.sigmoid(logits))\n",
    "    G_loss = lam * tf.abs(G_loss)\n",
    "    H_loss = - G_loss + \\\n",
    "            tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "    return H_loss, G_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_solvers(learning_rate=1e-2, beta1=0.8):\n",
    "    \"\"\"Create solvers for GAN training.\n",
    "    \n",
    "    Inputs:\n",
    "    - learning_rate: learning rate to use for both solvers\n",
    "    - beta1: beta1 parameter for both solvers (first moment decay)\n",
    "    \n",
    "    Returns:\n",
    "    - D_solver: instance of tf.train.AdamOptimizer with correct learning_rate and beta1\n",
    "    - G_solver: instance of tf.train.AdamOptimizer with correct learning_rate and beta1\n",
    "    \"\"\"\n",
    "    H_solver = tf.train.AdamOptimizer(learning_rate, beta1)\n",
    "    G_solver = tf.train.AdamOptimizer(learning_rate, beta1)\n",
    "    return H_solver, G_solver\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# number of images for each batch\n",
    "batch_size = 8192\n",
    "lam = 10\n",
    "\n",
    "# placeholder for images from the training dataset\n",
    "x = tf.placeholder(tf.float32, [None, 22])\n",
    "label = tf.placeholder(tf.float32, (None,1))\n",
    "ps = tf.placeholder(tf.float32, (None,1))\n",
    "# generated images\n",
    "G_sample = generator(x)\n",
    "logits = discriminator(G_sample)\n",
    "\n",
    "# Get the list of variables for the discriminator and generator\n",
    "H_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'discriminator')\n",
    "G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'generator') \n",
    "\n",
    "# get our solver\n",
    "H_solver, G_solver = get_solvers()\n",
    "\n",
    "# get our loss\n",
    "H_loss, G_loss = gan_loss(ps, label, logits, lam)\n",
    "\n",
    "# setup training steps\n",
    "H_train_step = H_solver.minimize(H_loss, var_list=H_vars)\n",
    "G_train_step = G_solver.minimize(G_loss, var_list=G_vars)\n",
    "H_extra_step = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'discriminator')\n",
    "G_extra_step = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_a_gan(sess, G_train_step, G_loss, H_train_step, H_loss, G_extra_step, H_extra_step,\\\n",
    "              show_every=250, print_every=40, batch_size=8192, num_epoch=400):\n",
    "\n",
    "    # compute the number of iterations we need\n",
    "    max_iter = int(credit_train.shape[0]*num_epoch/batch_size)\n",
    "    for it in range(max_iter):\n",
    "        # every show often, show a sample result\n",
    "        if it % show_every == 0:\n",
    "            pred = sess.run(tf.nn.sigmoid(logits), feed_dict={x:credit_test})\n",
    "            pred = np.asarray(pred)\n",
    "            sex_test = label_test[:,0]\n",
    "            def_test = label_test[:,1]\n",
    "            bias = np.mean(pred[sex_test==1]) - np.mean(pred[sex_test==2])\n",
    "            acc = np.mean((pred>0.5) == def_test)\n",
    "            print('Iter: {}, Acc: {:.4}, Bias:{:.4}'.format(it,acc,bias))\n",
    "        # run a batch of data through the network\n",
    "        indexes = np.random.choice(credit_train.shape[0], batch_size)\n",
    "        minibatch, minibatch_label = credit_train[indexes,:], label_train[indexes,1].reshape(-1,1)\n",
    "        props = p_scores[indexes,:]\n",
    "        _, H_loss_curr = sess.run([H_train_step, H_loss], \n",
    "                                  feed_dict={x: minibatch, \n",
    "                                             label: minibatch_label,\n",
    "                                             ps: props})\n",
    "        _, G_loss_curr = sess.run([G_train_step, G_loss], \n",
    "                                  feed_dict={x: minibatch, \n",
    "                                             label: minibatch_label,\n",
    "                                             ps: props})\n",
    "\n",
    "        # print loss every so often.\n",
    "        # We want to make sure D_loss doesn't go to 0\n",
    "        if it % print_every == 0:\n",
    "            print('Iter: {}, H: {:.4}, G:{:.4}'.format(it,H_loss_curr,G_loss_curr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Acc: 0.4801, Bias:5.633e-05\n",
      "Iter: 0, H: 0.6967, G:0.009761\n",
      "Iter: 40, H: 0.518, G:0.001156\n",
      "Iter: 80, H: 0.5094, G:0.00192\n",
      "Iter: 120, H: 0.5193, G:0.001156\n",
      "Iter: 160, H: 0.5294, G:0.002105\n",
      "Iter: 200, H: 0.5119, G:0.0002198\n",
      "Iter: 240, H: 0.515, G:0.001064\n",
      "Iter: 250, Acc: 0.7918, Bias:0.0003522\n",
      "Iter: 280, H: 0.5283, G:0.001137\n",
      "Iter: 320, H: 0.521, G:0.0008732\n",
      "Iter: 360, H: 0.533, G:0.0001033\n",
      "Iter: 400, H: 0.5302, G:0.0003207\n",
      "Iter: 440, H: 0.5236, G:0.000394\n",
      "Iter: 480, H: 0.5274, G:0.0008403\n",
      "Iter: 500, Acc: 0.7918, Bias:-0.0001589\n",
      "Iter: 520, H: 0.5328, G:0.001093\n",
      "Iter: 560, H: 0.5269, G:0.0006115\n",
      "Iter: 600, H: 0.5235, G:0.0002435\n",
      "Iter: 640, H: 0.5165, G:5.171e-05\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    run_a_gan(sess,G_train_step,G_loss,H_train_step,H_loss,G_extra_step,H_extra_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
